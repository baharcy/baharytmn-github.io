<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Machine learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">HOME</a></h1>
						<nav>
							<a href="#menu">Modules</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>MSc data science</h2>
							<ul class="links">
								<li><a href="index.html">Home</a></li>
								<li><a href="DBD.html">DECIPHERING BIG DATA</a></li>
								<li><a href="VD.html">VISUALISING DATA</a></li>
								<li><a href="NA.html">NUMERICAL ANALYSIS</a></li>
							
							</ul>
							<a href="#" class="close">Close</a>
						</div>
					</nav>

				<!-- Wrapper -->
					<section id="wrapper">
						<header>
							<div class="inner">
								<h2>MACHINE LEARNING</h2>
								<p>Demonstration of Projects, Reflections, and Other Artefacts: My Journey in Machine Learning.</p>
							</div>
						</header>
                                                          <!-- Content -->
							<div class="wrapper">
								<div class="inner">

									<h3 class="major">Unit 1 Collaborative Discussion I: The 4th Industrial Revolution</h3>
									<p> <h3>Initial Post</h3></p>
									<p>
										The Fourth Industrial Revolution (4IR) has profoundly transformed various industries, including finance, by integrating physical, digital, and biological realms through technologies such as AI, robotics, and blockchain (Salesforce, 2023). This transformation offers significant benefits, such as increased productivity, enhanced customer experiences, and personalised products and services. However, it also presents considerable challenges, particularly regarding cybersecurity (Global Banking and Finance, N.D.).
										
										Digital transformation initiatives in financial institutions have made it easier for cybercriminals to access highly sensitive data. A notable example is the 2017 Equifax data breach, which exposed sensitive information of about 147 million individuals (nearly 40% of the U.S. population), including identity details, social security numbers, and credit card information. The consequences were severe; affected customers faced increased risk of identity theft and fraud, resorting to measures like credit freezes and close financial monitoring. The breach also caused significant emotional distress and anxiety. For Equifax, the economic impact included a $700 million settlement, along with substantial reputational damage, eroding consumer trust and market position (Upguard, 2024; Wikipedia, 2024). The Equifax breach highlights the critical need for robust cybersecurity measures in line with the rapid advancements of 4IR technologies. Therefore, financial institutions must invest significantly in securing sensitive data to mitigate the risks associated with digital transformation.
										
										In summary, as Schwab (2016) notes, while 4IR brings numerous advantages, it also necessitates a balanced approach to innovation and security to manage the associated risks effectively.
										
										 
										
										
										
										</p>References: 
										
										</p>GlobalBankingandFinance (no date) The Fourth Industrial Revolution & the Impact on Banking & Finance (Challenges & Opportunities), globalbankingandfinance. Available at: https://www.globalbankingandfinance.com/the-fourth-industrial-revolution-the-impact-on-banking-finance-challenges-opportunities/ [Accessed: 04 August 2024]. 
										
										</p>Salesforce (2023) What is the Fourth Industrial Revolution?, Salesforce. Available at: https://www.salesforce.com/blog/what-is-the-fourth-industrial-revolution-4ir/ [Accessed: 03 August 2024]. 
										
										</p>Schwab, K. (2016) The Fourth Industrial Revolution. World Economic Forum. [Accessed: 03 August 2024]. 
										
										<p>Upguard (2024) 10 biggest data breaches in finance: Upguard, RSS. Available at: https://www.upguard.com/blog/biggest-data-breaches-financial-services [Accessed: 04 August 2024]. 
										
										<p>Wikipedia (2024) 2017 Equifax Data Breach, Wikipedia. Available at: https://en.wikipedia.org/wiki/2017_Equifax_data_breach#:~:text=Private%20records%20of%20147.9%20million,the%20public%20until%20September%202017. [Accessed: 04 August 2024]. </p>

									       
										<h3 class="major">Unit 1 Collaborative Discussion I: The 4th Industrial Revolution</h3>
										<p> <h3>Summary Post</h3></p>
										
										<p>Throughout the discussion regarding the Fourth Industrial Revolution (4IR), major incidents due to information systems failures and their consequences have been highlighted. Additionally, the discussion explored how 4IR is transforming industries by integrating physical, digital, and biological realms through technologies like AI, robotics, IoT, and blockchain (Schwab, 2016).

										In my initial post, I focused on the impact of 4IR on the finance sector, emphasising the cybersecurity challenges that arise from digital transformation. The Equifax data breach in 2017 served as an example, exposing the sensitive information of 147 million individuals and causing significant economic and reputational damage to the company (Upguard, 2024). This incident underscores the critical need for robust cybersecurity measures to accompany the rapid adoption of 4IR technologies.

										My peers contributed valuable insights to the discussion. Tala expanded on the growing need for data storage and management as Big Data continues to expand, particularly for financial institutions handling vast amounts of sensitive information. Aneil introduced the application of blockchain technology beyond finance, particularly in healthcare, highlighting the importance of considering environmental impacts and regulatory frameworks when adopting new technologies. Aminur emphasised the importance of human factors in cybersecurity, noting that human error remains a significant contributor to cyberattacks despite technological advancements.

										The discussion also aligned with the topics covered in Units 1, 2, and 3 of our ML module. Unit 1 introduced the timeline and the future of machine learning, which ties into how 4IR leverages AI and big data analytics to drive innovation. Unit 2 focused on exploratory data analysis (EDA), which is crucial in validating and preparing data for machine learning algorithms and making informed decisions. Lastly, Unit 3 on Correlation and Regression is relevant in understanding the relationships between variables.

										In summary, this discussion reflected the progress of 4IR, pointing out both opportunities and challenges in this transformation, while the three units provide essential knowledge about the key concepts driving this evolution.
										 
										
										
										
										</p>References: 
										
										</p>Schwab, K. (2016) The Fourth Industrial Revolution. World Economic Forum. [Accessed: 03 August 2024].  
										
										</p>Upguard (2024) 10 biggest data breaches in finance: Upguard, RSS. Available at: https://www.upguard.com/blog/biggest-data-breaches-financial-services [Accessed: 04 August 2024].  
									

										<p><h3>Peer Responses</h3></p>
										<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/DISCUSSION%201.pdf">View my peer responses</a></p>

										<section> <!-- Start of the section -->
    										<article> <!-- Unit 2 Article -->
      										<br><br> <!-- Adds two line breaks -->
        									<h3 class="major">Unit 2 Seminar Preparation - EDA Tutorial</h3>
        									<br><!-- Adds line breaks -->
										<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/UNIT%202%20EDA%20TUTORIAL.ipynb">View Python Code for Unit 2 Tutorial</a></p>
      										
										<p>In this tutorial, I conducted exploratory data analysis (EDA) on the Auto-mpg dataset, revealing that larger, heavier, and more powerful cars are less fuel-efficient (MPG), with positive correlations among these variables. Most variables showed positive skewness, indicating long right tails, while negative kurtosis suggested flatter distributions.</p>



   										</article>

    										<article> <!-- Unit 3 Article -->
        									<br><!-- Adds line breaks -->
        									<h3 class="major">Unit 3 Correlation and Regression</h3>
       									        <a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit03%20COMPONENT%20Ex%201%20covariance_pearson_correlation.ipynb">View Python Code for Unit03 COMPONENT Covariance Pearson Correlation</a>
       							                        <br><br> <!-- Adds two line breaks -->
										<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit03%20COMPONENT%20Ex2%20linear_regression.ipynb">View Python Code for Unit03 COMPONENT Linear Regression</a></p>
        									<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit03%20COMPONENT%20Ex3%20multiple_linear_regression.ipynb">View Python Code for Unit03 COMPONENT Multiple Linear Regression</a></p>
     										<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit03%20COMPONENT%20Ex4%20polynomial_regression.ipynb">View Python Code for Unit03 COMPONENT Polynomial Regression</a></p>
        									<p>In this activity, I ran programs on covariance, Pearson correlation, linear regression, multiple linear regression, and polynomial regression. In the first example, by adjusting variables, I observed that changing the standard deviation affects covariance, with higher variability reducing it, but it does not impact Pearson correlation since it measures the strength of the linear relationship independent of scale. Additionally, changes in the mean have no effect on either covariance or Pearson correlation, as both are based on deviations from the mean. In the second example, the linear regression model’s predictions and correlation can shift significantly when new data points, especially extreme values, are added to the dataset. The third example demonstrates a positive relationship between a car's weight and engine volume with CO2 emissions using a multiple linear regression model. In the last example, we used polynomial regression since the relationship between the independent variable (time) and the dependent variable (speed) is nonlinear.</p>
    										</article>
										</section> <!-- End of the section -->


        									<h3 class="major">Unit 4 Seminar Preparation - Linear Regression with Scikit-Learn</h3>
       									        <p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/UNIT%204%20Seminar%20Linear%20Regression%20with%20Scikit-Learn.ipynb">View Python Code for Unit04 Linear Regression with Scikit-Learn</a></p>
								    
									        <p>In this analysis, I used data from Global_Population.csv and Global_GDP.csv to explore the relationship between average population and per capita GDP across countries from 2001 to 2021. The results reveal a moderately positive correlation (0.712) and an R² of 0.506, indicating that the model explains 50.6% of the variance in GDP. However, the high mean squared error (MSE) suggests significant prediction errors, which may be due to missing explanatory variables.</p>

											
										<h3 class="major">Unit 5 Jaccard Coefficient Calculations</h3>
										<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Jaccard.pdf">View Solution for Unit05 Jaccard Coefficient Calculations</a></p>	
											
										<h3 class="major">Unit 5 Wiki Activity: Clustering</h3>
						                                <p>	
											
										The K-means clustering activity demonstrates the iterative nature of the algorithm, where data points are assigned to the nearest centroid, and these centroids are subsequently updated until convergence is achieved. This process illustrates how the initial placement of centroids significantly influences the final clustering results. A poor choice of starting points can lead to suboptimal clustering outcomes, underscoring the importance of careful initialisation in achieving effective clustering.	

										<h3 class="major">Unit 6 Seminar Preparation</h3>
						                                <p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit%206%20K-Means%20Clustering%20Iris%20Dataset.ipynb">View Solution for Unit06 K-Means Clustering Irıs Dataset</a></p>	
										<p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit%206%20K-Means%20Clustering%20Wine%20Dataset.ipynb">View Solution for Unit06 K-Means Clustering Wine Dataset</a></p>	

										<p>In this activity, I developed K-Means clustering models for both the Iris and Wine datasets using k = 3. Overall, K-Means performed reasonably well in both datasets, demonstrating clear success in distinguishing the more easily separable classes, such as Iris-setosa and Class 1 of the Wine dataset. However, the algorithm encountered challenges in accurately clustering instances from classes with overlapping features, notably Iris-versicolour and Iris-virginica, as well as between Class 2 and Class 3 in the Wine dataset. These results highlight the limitations of K-Means clustering in scenarios where the feature distributions of different classes are not well-separated.</p>
										
										<h3 class="major">Unit 7 Perceptron Activities</h3>
										<p>
										In a simple perceptron example, the sum function and the step function were utilized. The step function represents the simplest form of an activation function (GeeksforGeeks, 2024). This function determines whether a neuron is activated based on a specified threshold. I experimented with both positive and negative weights. When using positive weights, the resulting values exceeded 1, leading to the activation of the neuron. In contrast, when negative weights were applied, the output values were less than 1, resulting in the neuron remaining inactive.
										Single-layer perceptron can only solve linearly separable problems. However, the XOR problem is not linearly separable. Therefore, I use multi-layer architectures by incorporating hidden layers (GeeksforGeeks, 2024).
										In the perceptron AND operator example, the training process uses a step activation function to solve the XOR problem. It provides an illustration of basic ideas, including weight initialisation, error computation, and weight change during training.
										In the multilayer perceptron example, as an activation function, the sigmoid function is employed, where it ranges values from 0 to 1 to solve the XOR problem. It is aiming to show error computation and backpropagation for weight adjustment and training over multiple epochs to minimise the output error.

										
										 
										
										
										
										</p>References: 
										
										</p>GeeksforGeeks (2024). Activation functions. https://www.geeksforgeeks.org/activation-functions/ [Accessed: 16 October 2024].
										
										</p>GeeksforGeeks (2024) How neural networks solve the XOR problem, GeeksforGeeks. Available at: https://www.geeksforgeeks.org/how-neural-networks-solve-the-xor-problem/ [Accessed: 16 October 2024].</p>
											

										<h3 class="major">Unit 8 Gradient Cost Function</h3>	
						                                <p>In the gradient cost function activity, I explored two crucial parameters: iterations and learning rate. Iterations refer to the number of times the algorithm updates its parameters based on the training data, while the learning rate determines the magnitude of the weight adjustments during each update in the training process. I discovered that increasing iterations generally leads to a reduction in the cost function; however, a high learning rate can produce meaningless outputs. Therefore, balancing these factors is essential for optimising performance and ensuring accurate results in machine learning algorithms.</p>	



										<h3 class="major">Unit 8 Collaborative Discussion II: Legal and Ethical views on ANN applications</h3>
									        <p> <h3>Initial Post</h3></p>
									        <p>
										In recent years, the rise of AI writers, particularly AI chatbots, has led to significant breakthroughs in automated content generation. These AI writers assist with a wide range of tasks, including both administrative and creative writing. Their advantages include increased efficiency, cost savings, and improved comprehension levels, making them appealing to many organisations.
										However, while AI writers offer several advantages, they also raise serious ethical concerns that must be addressed. Critics have described AI writers as “stochastic parrots,” reflecting concerns that this technology may contribute to a less creative work environment (Hutson, 2021; Algolia, 2024).
										In the realm of scientific writing, AI chatbots play an essential role in assisting researchers by organising information, generating initial documents, and proofreading. Nevertheless, the risk of inaccuracies and plagiarism persists, which can have serious implications for research integrity. Moreover, as these tools may transition to paid services, disparities in access could widen the gap between wealthier and poorer nations (Salvagno et al., 2023). In education, while AI writing tools can enhance the learning experience by generating personalised materials and helping students structure their writing, over-reliance on these systems may impede the development of critical thinking and writing skills (Shidiq, 2023).
										Other sectors significantly impacted by AI writing technology include media and entertainment, advertising, healthcare, and finance. While these sectors benefit from increased innovation, they must also address the ethical challenges associated with AI adoption (Nas, 2023).
										In summary, AI writers serve as a double-edged sword; while they enhance productivity and creativity, they also pose risks that require careful consideration and regulatory frameworks.

										
										 
										
										
										
										</p>References: 
										
										</p>Algolia. (2024). AI language models: The good, the bad, and the ugly. https://www.algolia.com/blog/ai/the-pros-and-cons-of-ai-language-models/ [Accessed: 05 October 2024]. 
										
										</p>Hutson, M. (2021). Robo-writers: The rise and risks of language-generating AI. Nature News. https://www.nature.com/articles/d41586-021-00530-0 [Accessed: 05 October 2024]. 
										
										</p>Nas, D. (2023). 5 sectors that will be most affected by Generative AI. Medium. https://deborahnas.medium.com/5-sectors-that-will-be-most-affected-by-generative-ai-fd4c23bc3b0 [Accessed: 05 October 2024].
										
										</p>Salvagno, M., Taccone, F. S., & Gerli, A. G. (2023). Can artificial intelligence help for scientific writing? Critical Care, 27(1), 75 [Accessed: 04 October 2024].
										
										</p>Shidiq, M. (2023). The use of artificial intelligence-based ChatGPT and its challenges for the world of education; from the viewpoint of the development of creative writing skills. In Proceeding of International Conference on Education, Society, and Humanity (Vol. 1, No. 1, pp. 353-357) [Accessed: 05 October 2024].</p>
	
											
										<h3 class="major">Unit 9 CNN Model Activity</h3>
									        <P>
										Wall (2019) highlights the racial bias present in facial recognition technology, raising significant ethical concerns. Yao (2024) supports this perspective by demonstrating that facial recognition systems exhibit higher error rates for minorities, which can lead to unfair treatment. Yao addresses privacy concerns associated with advanced deep learning technologies, particularly the potential for unauthorised tracking in public spaces, which cause violations of individuals' rights to privacy. These ethical considerations must be prioritised in the development and application of CNN technology in facial recognition.
										In reviewing the CNN code, I observed the importing of the CIFAR-10 dataset and the necessary data pre-processing steps, including normalisation of pixel values and categorical encoding to prepare the data for training. Another crucial step is creating a validation set to assess the model's performance effectively. The CNN model is then built, with the early stopping callback feature added to prevent overfitting. After completing the evaluation process, I changed the input image and successfully obtained the output “cat.”

										
										
										</p>References: 
										
										</p>BBC News. (2019, July 8). Biased and wrong? Facial Recognition Tech in the dock. https://www.bbc.com/news/business-48842750 [Accessed: 16 October 2024].
										
										</p>Yao, Y. (2024). The Impact of Deep Learning on Computer Vision: From Image Classification to Scene Understanding. Valley International Journal Digital Library, 1428-1433 [Accessed: 16 October 2024].<p>
											
										<h3 class="major">Unit 10 Seminar Preparation - CNN Tutorial</h3>
       									        <p><a href="https://github.com/baharcy/baharytmn-github.io/blob/main/Unit10%20CNN%20Tutorial.ipynb">View Python Code for Unit10 CNN Tutorial</a></p>
								    
									        <p>In this exercise, I explored how convolutional neural networks (CNNs) process images using the pre-trained VGG16 model. VGG16, a popular deep learning architecture for image classification, has been trained on the ImageNet dataset, which contains millions of labelled images. The primary objective was to understand how CNNs progressively extract features from images across different layers. Early layers capture basic patterns like edges and textures, while deeper layers focus on more complex structures and high-level object representations. This investigation provided insights into the hierarchical nature of CNNs and how they transform raw pixels into meaningful features for classification.</p>
	
										</p>Reference:

									        </p>arXiv.org. (2015, April 10). Very deep convolutional networks for large-scale image recognition. https://arxiv.org/abs/1409.1556<p>
											
										<h3 class="major">Unit 11 Model Performance Measurement</h3>
       									
											
									        <p>The Support Vector Classifier is a supervised learning algorithm used for classification tasks which finds the optimal hyperplane that separates data into distinct classes by maximising the margin between the hyperplane and the closest data points. This approach helps the model generalise well to unseen data (TechTarget, 2023). In this activity, I evaluated the SVC's performance using AUC and R-squared metrics and it can be said increasing the regularisation parameter C led to improved model performance, reflected in higher AUC and R² scores.</p>	
											
										</p>Reference:
									
										</p>TechTarget (2023) What is a support vector machine?: Definition from Whatis, TechTarget. Available at: https://www.techtarget.com/whatis/definition/support-vector-machine-SVM#:~:text=A%20support%20vector%20machine%20(SVM)%20is%20a%20type%20of%20supervised,data%20set%20into%20two%20groups [Accessed: 16 October 2024].</p> 	
											
										
										<h3 class="major">Unit 12 Future of Machine Learning</h3>
											
										<p>According to Diez-Olivan et al. (2019), Industry 4.0 enables the extraction of valuable insights from industrial assets using data fusion and machine learning techniques. The paper categorises data-driven industrial prognosis into three types: descriptive prognosis, which analyses the root causes of failures; predictive prognosis, which forecasts when failures will occur; and prescriptive prognosis, which recommends actions to minimise the impact of failures.
										In the finance sector, predictive prognostic models play a key role in forecasting future market conditions. By leveraging extensive historical data and advanced analytical techniques, these models enable financial institutions to assess and mitigate potential risks. Furthermore, they maximise returns by guiding firms in making informed, data-driven investment decisions (Investopedia,N.D) . Ultimately, predictive models empower organisations to proactively anticipate market changes, gaining a competitive advantage as the sector transforms.</p>


										</p>References: 
											
										</p>Diez-Olivan, A., Del Ser, J., Galar, D., & Sierra, B. (2019). Data fusion and machine learning for industrial prognosis: Trends and perspectives towards Industry 4.0. Information Fusion, 50, 92-111.
											
										</p>Investopedia (no date) What is predictive modeling?, Investopedia. Available at: https://www.investopedia.com/terms/p/predictive-modeling.asp [Accessed: 17 October 2024].<p> 
	
										
										<h3 class="major">Reflective Piece</h3>	

										<p>In this module, I have significantly expanded my understanding of machine learning algorithms, gaining valuable hands-on experience through two key projects. This has led me to reflect on how these newly acquired skills will influence both my professional and personal growth. Moreover, the module provided a critical bridge between theory and practice, deepening my understanding of how technological advancements influence artificial intelligence and machine learning today. This experience has enhanced my ability to apply theoretical concepts to real-world applications, equipping me to tackle future challenges in this rapidly evolving field.

										One of the most rewarding aspects of this module was the opportunity to explore supervised, reinforcement, and unsupervised learning models, with a particular focus on reinforcement learning and deep learning techniques. I had the chance to experiment with these models during our presentation project, which involved the CIFAR-10 dataset. This practical experience allowed me to work with artificial neural networks, utilising TensorFlow and Keras libraries to model convolutional neural networks, determine the number of epochs, select features using activation and loss functions, and evaluate and optimise the models. Initially, I found these tools and techniques challenging, but they significantly enhanced my Python programming skills. Implementing neural networks practically rather than just learning the theory was transformative, especially in light of their real-world applications. Additionally, presenting via Zoom for the first time was a valuable experience, as it will benefit my future studies and career. Developing presentation skills is essential for professional growth, and this experience has helped me build confidence in this area.

										Moreover, I expanded my knowledge of unsupervised learning by delving into K-means clustering and its extension, K-prototype clustering, which we employed in our development project. This algorithm is designed for datasets containing both numerical and categorical variables (Medium, 2023). Acquiring this knowledge was crucial for dealing more complex datasets in the future, where traditional clustering algorithms like K-means would be insufficient. As the person responsible for the exploratory data analysis (EDA) process, I organised and analysed our data in preparation for machine learning model building. I explored the dataset in-depth, visualising patterns, identifying missing values, and ensuring that the data was ready for modelling. This meticulous attention to detail was essential, as the quality of the EDA directly influenced the success of the models we later implemented (Verpex, 2024). In addition to enhancing my technical skills, this team project significantly improved my ability to collaborate effectively. I also took on the responsibility of task delegation within our group, a new challenge that involved ensuring every team member had a fair workload while maintaining overall project progress. This experience has motivated me for future projects and underscored the importance of clear communication.

										In terms of unit coverage, I was already familiar with the initial units from my previous background and earlier modules in this program, particularly topics like correlation, regression, and the exploratory data analysis process. However, I enhanced my skills by utilising Python more extensively, for example, by applying Scikit-Learn for regression analysis. Additionally, clustering, which was new to me, proved to be a challenging yet rewarding experience. The lecturecasts, combined with practical portfolio activities, helped me grasp the fundamentals of clustering machine learning models. Using Python to implement these concepts deepened my understanding. This practical exposure was crucial for my comprehension of how these models function and their real-world applications.


										Overall, the projects I completed, along with the materials from each unit and the artefacts in my portfolio, have given me a comprehensive foundation in machine learning. Key activities that significantly contributed to my learning include K-means clustering, convolutional neural network models, and regression analysis. Together, these efforts have established the groundwork for building and understanding machine learning models. Additionally, engaging in two discussions during this period allowed me to explore the Fourth Industrial Revolution and technological advancements more deeply, particularly their broader implications and consequences.

										I had the opportunity to attend several seminars that provided not only module-specific guidance and assignment support but also insights into significant developments in the field. These sessions highlighted key figures in AI and machine learning that are essential to know. Additionally, we were introduced to valuable tools for our careers, such as the Paperpile reference manager, which will be particularly beneficial for me as a new PhD student. These experiences have broadened my perspective and equipped me with practical resources for both academic and professional growth.

										Additionally, this is my second time creating an e-portfolio, and I feel I have made significant progress in documenting my development. This improved documentation process will be valuable for both my future academic and professional pursuits. I have also begun to develop proficiency in GitHub, which is a crucial platform for anyone working in the tech world. Mastering this tool will not only enhance my ability to showcase my projects but also strengthen my collaboration skills within the technology communities.


										This module has inspired a deep curiosity in technological advancements, particularly in Industry 4.0 and artificial neural networks. It has increased my awareness of the ethical, social, and professional implications of machine learning technologies, emphasising the responsible use of AI. Furthermore, the module has motivated me to explore more advanced AI applications. I have gained greater confidence in tackling future projects by overcoming challenges such as implementing complex neural network algorithms and understanding evaluation metrics. This experience has also fostered resilience and adaptability. Additionally, preparing my e-portfolio and managing two projects sharpened my organizational and time management skills under tight deadlines, offering both challenges and fulfilling rewards. Moving forward, I am determined to build on this foundational knowledge and enhance my skills in AI implementation. As a result, this module has not only broadened my professional perspective but has also influenced my career path significantly. I have now embarked on a PhD program to further my expertise in AI.</p>  

	

										</p>References: 

										</p>Geeksforgeeks (2024). Clustering in machine learning. https://www.geeksforgeeks.org/clustering-in-machine-learning/  [Accessed: 18 October 2024].

										</p>Medium (2023) K-Prototypes & other statistical techniques to cluster with categorical and numerical features: A... https://juandelacalle.medium.com/k-prototypes-other-statistical-techniques-to-cluster-with-categorical-and-numerical-features-a-ac809a000316  [Accessed: 17 October 2024].<p>
										
										</p>Verpex (2024). Eda in machine learning. https://verpex.com/blog/website-tips/eda-in-machine-learning [Accessed: 17 October 2024].<p>
										
										
									           <!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
